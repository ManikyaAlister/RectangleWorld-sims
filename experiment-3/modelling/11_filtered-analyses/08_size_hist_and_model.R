###### Frequency of rectangles by size #########
# This is a mess

rm(list = ls())
# setup F
library(here)
source(here("getLearnerHypDistributions.R"))
source(here("calculatingFunctions.R"))
source(here("plottingFunctions.R"))

# load generic data
load(here("experiment-3/data/derived/all_conditions.Rdata"))
source(here("getLearnerHypDistributions.R"))
load(here(
  "experiment-scenarios/target-blocks/data/target-block-8-Cartesian.Rdata"
))
observations = targetBlock$observations

load(here("experiment-3/modelling/11_filtered-analyses/good_uid_liberal.Rdata"))

# load participant data
load(here("experiment-3/data/derived/data_cartesian.Rdata"))

# filter data 
d_cartesian <- d_cartesian %>%
  filter(pid %in% good_uid_liberal)

# function for scaling rectangle sizes based on how many rectangles of a size there is
scaled_rectangle_sizes <- function(data, hyp) {
  # Rename column for consistency with data
  hyp$size_resp <- as.character(hyp$size)
  
  # Calculate the frequency of each drawn rectangle size
  drawn_rect_freq <- data %>%
    group_by(size_resp) %>%
    summarise(drawn_count = n())
  
  hyp_freq <- hyp %>%
    group_by(size_resp) %>%
    summarise(drawn_count = n()) %>%
    mutate(size_resp = as.numeric(size_resp))
  
  # Merge drawn rectangle frequencies with hyp data to get possible rectangle counts
  combined_data <- merge(drawn_rect_freq, hyp_freq, by.x = "size_resp", by.y = "size_resp", all.x = TRUE)
  
  # Calculate the scaled frequencies
  combined_data$scaled_freq <- combined_data$drawn_count.x / combined_data$drawn_count.y
  combined_data
}


# Model predictions -------------------------------------------------------

alphas <- c(-1, 0, 1) # alphas we are interested in modelling
c <- 3 # clue we are interested in
b <- 8 # block we are interested in
targetBlocks <- c(2,8)


all_conditions_filtered <- all_conditions %>%
  filter(clues == c & blocks == b)
nConds <- length(all_conditions_filtered[, 1])

# Predicted distributions for each alpha
all_dists = NULL
for (i in 1:length(alphas)) {
  dist <-
    getLearnerHypDistribution(observations, alpha = alphas[i], prior = "flat")
  # convert to df (probably a better way than this)
  distDf <- NULL
  # loop through clues if more than one clue to be plotted
  for (j in c) {
    distBlock <- dist[[j]]
    distDf <- rbind(distDf, distBlock)
    if (alphas[i] > 0) {
      distDf$cond <- "helpful"
    } else if (alphas[i] == 0) {
      distDf$cond <- "random"
    } else {
      distDf$cond <- "misleading"
    }
  }
  all_dists <- rbind(all_dists, distDf)
}

# uninformative condition (recursive learner, deception)
dist <-
  getLearnerHypDistribution(observations,
                            alpha = -2,
                            prior = "flat",
                            recursion = TRUE)

# convert to DF
distDf <- NULL
for (j in c) {
  distBlock <- dist[[j]]
  distDf <- rbind(distDf, distBlock)
}
distDf$cond <- "uninformative"

# combine all conditions
all_dists <- rbind(all_dists, distDf)

# get probability distributions
all_dists <- all_dists %>%
  mutate(Alpha = as.factor(alpha)) %>%
  mutate(
    clue_name = case_when(
      clue == 1 ~ "1st Clue",
      clue == 2 ~ "2nd Clue",
      clue == 3 ~ "3rd Clue",
      clue == 4 ~ "4th Clue"
    )
  )


# Participant responses ---------------------------------------------------

# Need to make a single data frame with participant response, size, and probability.
# get probability of a certain sized rectangle under a certain alpha

d_sizes <- all_dists %>%
  group_by(cond, size) %>%
  summarise(prob = median(posterior))
d_sizes$count = 0

# function to help match the frequency of rectangles generated by a participant with that rectangle's probability
replace_count <- function(d_size, d_cond) {
  for (i in 1:nrow(d_size)) {
    size_match <- d_cond$size_resp == d_size$size[i]
    if (any(size_match)) {
      d_size$count[i] <- d_cond$count[which(size_match)]
    }
  }
  return(d_size)
}

d_all_sizes <- NULL



# Populate the "count" column in d_sizes for each condition, then make a single
# large data frame with sizes, probability, alpha, and frequency for each  condition
for (i in 1:nConds) {
  condition <- all_conditions_filtered[i, "conditions"]
  
  d_cond <- d_cartesian %>%
    filter(block == b & clue == c & cond == condition) %>%
    group_by(size_resp) %>%
    summarise(count = n())
  
  # add column for experimental condition
  d_cond$cond <- condition
  
  # add column for cover story/model condition
  d_cond <- d_cond %>%   mutate(
    cover_cond = case_when(
      cond == "HS" | cond == "HN" ~ "helpful",
      cond == "MS" | cond == "MN" ~ "misleading",
      cond == "UN" | cond == "US" ~ "uninformative",
      cond == "RS" | cond == "RN" ~ "random"
    )
  )
  
  
  d_sizes_cover_cond <-
    d_sizes[d_sizes[, "cond"] == unique(d_cond$cover_cond), ]
  
  
  d_cond_sizes <- replace_count(d_sizes_cover_cond, d_cond)
  d_cond_sizes$cond <- condition
  d_all_sizes <- rbind(d_all_sizes, d_cond_sizes)
}

# add column for cover story/model condition
all_conditions_filtered <- all_conditions_filtered %>%
  mutate(
    plotCond = case_when(
      conditions == "HS" | conditions == "HN" ~ "helpful",
      conditions == "MS" | conditions == "MN" ~ "misleading",
      conditions == "UN" | conditions == "US" ~ "uninformative",
      conditions == "RS" | conditions == "RN" ~ "random"
    )
  )

# Plot
# loop through all conditions and make a plot for each

for (i in 1:length(all_conditions_filtered[, 1])) {
  condition <- all_conditions_filtered[i, "conditions"]
  plotCond <- all_conditions_filtered[i, "plotCond"]
  
  if (condition == "US" | condition == "UN") {
    sizeHistModel(d_all_sizes, condition, plotCond, prob_constant = 100, ylim = 65)
  } else {
    sizeHistModel(d_all_sizes, condition, plotCond, prob_constant = 400, ylim = 65)
  }
  ggsave(filename = here(
    paste0(
      "experiment-3/modelling/11_filtered-analyses/plots/size_hist_b8_c",
      c,
      "_",
      condition,
      ".png"
    )
  ),
  width = 7,
  height = 2.3)
}


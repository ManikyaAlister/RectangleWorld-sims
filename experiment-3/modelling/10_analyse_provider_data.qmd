---
title: "Provider Analyses"
format:
  html:
    code-fold: true
    code-summary: "Show the code"
editor: visual
---

## 

```{r include=FALSE}
library(here)
library(tidyverse)
library(ggpubr)

source(here("plottingFunctions.R"))
source(here("calculatingFunctions.R"))
```

```{r}
# set up file directory
exp <- 3
directory <- paste0("experiment-",exp,"/modelling/04_output/provider-scores/")
file_list <- list.files(here(directory))
file_list_ranked <- file_list[str_detect(file_list, "ranked")]
file_list_prob <- file_list[file_list != file_list_ranked]


load(here("experiment-3/data/clean/data_teaching_cartesian.Rdata"))
learner_conds <- d_cartesian_t %>%
  group_by(pid, cond) %>%
  summarise()

#load(here("experiment-3/data/derived/provider_scores_pilot.rdata"))
loadAndCombine = function(file_list, directory, learner_conds){
  combined_df <- NULL
  
  # load all data files and combine into single data frame
  for (i in 1:length(file_list)){
    file_name <- file_list[i]
    load(here(paste0(directory,file_name)))
    p <- unique(d_iteration$uid)
    learn_cond <- as.character(learner_conds[learner_conds[,"pid"] == p,"cond"])
    
    if (learn_cond == "HS"){
      learn_cond <- "helpful"
    } else if (learn_cond == "RS") {
      learn_cond <- "random"
    } else if (learn_cond == "MS"){
      learn_cond <- "misleading"
    } else if (learn_cond == "US"){
      learn_cond <- "uninformative"
    }
    
    d_iteration$learn_cond <- learn_cond
    combined_df <- rbind(combined_df, d_iteration)
  }
  
  unique_pids <- unique(combined_df$uid)
  pid_mapping <- setNames(1:length(unique_pids), unique_pids)
  combined_df$p_num <- match(combined_df$uid, names(pid_mapping))
  combined_df$p_num <- factor(combined_df$p_num)
  combined_df
}

provider_scores_all <- loadAndCombine(file_list_prob, directory, learner_conds)
provider_scores_ranked_all <- loadAndCombine(file_list_ranked, directory, learner_conds)

provider_scores <- provider_scores_all %>%
  filter(model == provider_cond)

provider_scores_ranked <- provider_scores_ranked_all %>%
  filter(model == provider_cond)
```

## Participant performance in each provider condition

```{r}
#| warning=FALSE

# inspect provider scores on their own
plotProviderScores = function(provider_scores, ranked = FALSE){
  
  sum_provider_scores <- provider_scores %>%
    group_by(p_num, provider_cond) %>%
    summarise(prob= mean(as.numeric(prob))) 
if (ranked) {
  plot <- provider_scores %>% 
    ggplot(aes(x = p_num, y = 100-as.numeric(prob)))+
    geom_col(data = sum_provider_scores)+
    geom_point(aes(colour = size))+
    labs(y = "Reverse rank (100 = best)", title = "Rank of participant responses according to model")+
    facet_wrap(~provider_cond)
} else {
  plot <- provider_scores %>% 
    ggplot(aes(x = p_num, y = as.numeric(prob)))+
    geom_col(data = sum_provider_scores)+
    geom_point(aes(colour = size))+
        labs(y = "Probability", title = "Probability of points chosen by participants according to the model for that condition")+
    facet_wrap(~provider_cond)
}
  
  
  plot
}

plotProviderScores(provider_scores)

```

Because there is higher posterior density allocated to certain points in different conditions, it makes sense to look at this in an ordinal way. In other words, what rank was the point they chose relative to all of the possible points? In the interest of keeping the plots consistent, I have reverse ordered the plot below so that "100" is the best possible point they could have chosen according to the model, and 1 is the worst.

```{r}
#| warning=FALSE
plotProviderScores(provider_scores_ranked, ranked = TRUE)
```

## Correlation between provider and learner phases

Is there a relationship between how well a participant can provide clues in a given condition with how they can learn the rectangle?

```{r}
#| warning = FALSE

getProviderLearnerCorr = function(provider_scores, alpha, blocks = 8, recursive = FALSE, main = ""){
  
  if (alpha > 0 ){
    cond <- "helpful"
  } else if (alpha == 0){
    cond <- "random"
  } else if (alpha < 0 & recursive == TRUE) {
    cond <- "misleading"
  } else {
    cond <- "uninformative"
  }
  
  learner_posteriors <- NULL
  
  for (b in blocks) {
    load(here(paste0("experiment-3/modelling/04_output/b",b,"-all-alpha-posteriors-",cond,".Rdata")))
    learner_posteriors <- rbind(learner_posteriors, all_alpha_posteriors)
  }
  # get participants who were in each condition
  participants <- unique(learner_posteriors$pid)
  
  p_provider <- provider_scores %>%
    filter(uid %in% participants & provider_cond == cond) %>%
    # get 1 score for each participant
    group_by(uid)%>%
    summarise(mean = mean(prob))
  
  p_learner <- learner_posteriors %>%
    filter(alpha == alpha) %>%
    group_by(pid)%>%
    summarise(mean = mean(posterior))
  print(cor(p_provider$mean, p_learner$mean))  
  plot(p_provider$mean, p_learner$mean, main = main)  
}

# load learning phase data
blocks <- 8

```

### Raw probabilities

```{r}

getProviderLearnerCorr(provider_scores, -1, blocks = blocks, recursive = FALSE, main = "Helpful")

getProviderLearnerCorr(provider_scores, -1, blocks = blocks, recursive = FALSE, main = "Misleading")

getProviderLearnerCorr(provider_scores, -1, blocks = blocks, recursive = TRUE, main = "Uninformative")

```

### Ranks

```{r}
getProviderLearnerCorr(provider_scores_ranked, -1, blocks = blocks, recursive = FALSE, main = "Helpful")

getProviderLearnerCorr(provider_scores_ranked, -1, blocks = blocks, recursive = FALSE, main = "Misleading")

getProviderLearnerCorr(provider_scores_ranked, -1, blocks = blocks, recursive = TRUE, main = "Uninformative")

```

## Filtering participants based on performance in learning phase

If we only include participants who performed in line with model predictions in the provider phase, how does that influence the group-level learning phase results?

```{r}
#| warning: false
score_by_model <- provider_scores_all %>%
  #filter(provider_cond == learn_cond) %>%
  group_by(p_num, provider_cond, model) %>%
  summarise(score = median(prob))

score_by_model %>%
  ggplot(aes(x = p_num, y = score, fill = model))+
  geom_bar(position="dodge", stat="identity")+
  labs(title = "Provider score according to each model as a function of condition")+
  facet_wrap(~provider_cond)
```

Random always does well in the ranked comparison because everything is rank 1, so let's remove that here.

```{r}
#| warning: false
score_by_model_rank <- provider_scores_ranked_all %>%
  filter(model != "random") %>%
  group_by(p_num, provider_cond, model) %>%
  summarise(score = median(100-prob))


score_by_model_rank %>%
  ggplot(aes(x = p_num, y = score, fill = model))+
  geom_bar(position="dodge", stat="identity")+
  labs(title = "Provider score according to each model as a function of condition")+
  facet_wrap(~provider_cond)
```

Now let's look at quantitatively at which participants behaved most like each model in the respective conditions.

### Libearl filtering: Remove participants who act helpful all the time.

Liberal (more relaxed) filtering: filter based on 1. If the best model in the helpful condition was the helpful model and 2. If the best model in the misleading condition was *not* the helpful condition. I think it's a good idea to remove the uninformative condition here, because the uninformative model highly resembles the helpful model, so it might not exclude participants who are just acting like a helpful provider.

```{r}
# Figure out which model performed best for each participant and techer condition
getBestProviderModel = function(scores_by_model, models){
  # Convert to wide format
scores_by_model_wide <- scores_by_model %>%
  pivot_wider(
    id_cols = c(p_num, provider_cond),
    names_from = model,
    values_from = score
  )

# make temporary df with unnecessary columns
tmp <- scores_by_model_wide[,models]

best_models <- apply(tmp, 1, function(row) {
  models[which.max(row)]
})

scores_by_model_wide$best_model <- best_models
scores_by_model_wide
}

models <- unique(provider_scores_all$model)

best_models <- getBestProviderModel(score_by_model, models) 

# Liberal filtering: filter based on 1. If the best model in the helpful condition was the helpful model and 2. If the best model in the misleading condition was *not* the helpful condition. 

good_subj_liberal <- best_models %>% 
  # because uninformative and helpful often mimick each other, removing uninformative. 
  filter(provider_cond != "uninformative") %>%
  mutate(match = case_when(provider_cond == "helpful" & best_model == "helpful" ~ TRUE,
                           provider_cond == "misleading" & best_model != "helpful" ~ TRUE,
                          #provider_cond == "uninformative" & best_model != "helpful" ~ TRUE,
                           TRUE ~ FALSE
                           )) %>%
  group_by(p_num) %>%
  summarise(sum = sum(match)) %>%
  filter(sum == 2)

vec_good_subj_liberal <- unique(good_subj_liberal$p_num)

print(c("Participants who met the liberal provider phase exclusion criteria ", vec_good_subj_liberal)) 


```

### Plot learner behavior

```{r}
#| warning: false
#load leaner data 
load(here("experiment-3/data/derived/data_cartesian.Rdata"))

# load target block data
load(here("experiment-scenarios/target-blocks/data/target-block-8-Cartesian.Rdata"))

# load experiment condition data
load(here("experiment-3/data/derived/all_conditions.Rdata"))

# filter conditions of interest-- let's just look at the second target block for now. 
exp_params <- all_conditions %>%
  filter(blocks == 8, clues %in% 1:4) 

# get participant IDs
PID <- unique(d_cartesian$pid)

# get participant IDs who passed liberal exclusion criteria
good_ps <- PID[vec_good_subj_liberal]

data_g <- d_cartesian %>%
  filter(pid %in% good_ps)

getHypProbs(d = data_g, all_conditions = exp_params, experiment = 3, file_label = "-filtered-lib")

plotHeatMaps(all_conditions = exp_params, experiment = 3, save = FALSE, file_label = "-filtered-lib")

```

### Conservative filtering: Model must match condition

Filter to only include participants whose best model matched the provider condition they were in.

```{r}


good_subj_conservative <- best_models %>% 
  mutate(match = case_when(provider_cond == best_model ~ TRUE,
                           TRUE ~ FALSE
                           )) %>%
  group_by(p_num) %>%
  summarise(sum = sum(match)) %>%
  filter(sum == 3)

vec_good_subj_conservative <- unique(good_subj_conservative$p_num)

print(c("Participants who met the conservative provider phase exclusion criteria ", vec_good_subj_conservative)) 

```

### Repeat for ranked data

```{r}
models <- unique(score_by_model_rank$model)


best_models <- getBestProviderModel(score_by_model_rank, models) 

# Liberal filtering: filter based on 1. If the best model in the helpful condition was the helpful model and 2. If the best model in the misleading condition was *not* the helpful condition. 

good_subj_liberal <- best_models %>% 
  # because uninformative and helpful often mimick each other, removing uninformative. 
  filter(provider_cond != "uninformative") %>%
  mutate(match = case_when(provider_cond == "helpful" & best_model == "helpful" ~ TRUE,
                           provider_cond == "misleading" & best_model != "helpful" ~ TRUE,
                           TRUE ~ FALSE
                           )) %>%
  group_by(p_num) %>%
  summarise(sum = sum(match)) %>%
  filter(sum == 2)

vec_good_subj_liberal <- unique(good_subj_liberal$p_num)

print(c("Participants who met the liberal provider phase exclusion criteria ", vec_good_subj_liberal)) 

```

```{r}

good_subj_conservative <- best_models %>% 
  mutate(match = case_when(provider_cond == best_model ~ TRUE,
                           TRUE ~ FALSE
                           )) %>%
  group_by(p_num) %>%
  summarise(sum = sum(match)) %>%
  filter(sum == 3)

vec_good_subj_conservative <- unique(good_subj_conservative$p_num)

print(c("Participants who met the conservative provider phase exclusion criteria ", vec_good_subj_conservative)) 
```

---
title: "Pedagogical Learner"
author: "Manikya Alister"
date: "16/08/2022"
output: 
  html_document:
    theme: simplex
    code_folding: hide
---
```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
library(here)
source(here("least-squares-heuristic.R"))
source(here("pedagogical-learner.R"))
source(here("functions.R"))
```

## Set up rectangle world 

```{r include = TRUE}

# Define range of possible features
range = 1:10

# set up all possible hypotheses
borders = makeBorders(range)

# Define the true category region
cat1 = c(2,2,6,8)

#visualize true category region
plot(c(1,max(range)), c(1, max(range)), type= "n", xlab = "", ylab = "")
rect(cat1[1],cat1[2],cat1[3],cat1[4],border = "red", lwd = 3)
```

## Sample some observations

For this simulation I'm going to do a maximum of 10 observations, 5 positive examples and 5 negative examples. I'm interested to see how the different learning strategies (weak, strong, pedagogical) generalise given different quantities of examples (e.g., 1, 3, 5 of each positive/negative evidence type). 

This sampling is, I guess, a form of weak sampling, but it's not truly weak because I'm ensuring an equal number of positive and negative examples. 

```{r echo = TRUE}
nObs = 10
# Positive examples
pX = round(runif(nObs/2, cat1[1],cat1[3]),2) # X coordinates 
pY = round(runif(nObs/2, cat1[2],cat1[4]),2) # Y coordinates
pos = cbind(pX,pY,"positive")

## Need to find a  better way to sample negative evidence 
neg = weakSampler(20)
neg = neg[neg[,"category"]== "negative",]
neg = neg[1:5,]

# set up different arrays with different numbers of observations
obs1 = rbind(pos[1,],neg[1,])
colnames(obs1) = c("x","y","category")

obs3 = rbind(pos[1:3,],neg[1:3,])
colnames(obs3) = c("x","y","category")

obs5 = rbind(pos[1:5,],neg[1:5,])
colnames(obs5) = c("x","y","category")

# visualize observations
plot(c(1,max(range)), c(1, max(range)), type= "n", xlab = "", ylab = "", main = "Hypothesis Space, Observations, and True Category Boundary")
rect(cat1[1],cat1[2],cat1[3],cat1[4],border = "red", lwd = 2)
points(obs5)
```

```{r echo = FALSE}
weak1 = pedLearner(borders, obs1, alpha = 0)
weak3 = pedLearner(borders, obs3, alpha = 0)
weak5 = pedLearner(borders, obs5, alpha = 0)
```

```{r echo = FALSE}
strong1 = pedLearner(borders, obs1)
strong3 = pedLearner(borders, obs3)
strong5 = pedLearner(borders, obs5)
```

```{r}
ls1 = lsLearner(borders, obs1)
ls3 = lsLearner(borders, obs3)
ls5 = lsLearner(borders, obs5)
```


```{r}
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))

plotPed(weak1,obs1,cat1)
mtext("Weak Learner a = 0", side = 3)

plotPed(strong1, obs1, cat1)
mtext("Pedegogical Learner a = 1", side = 3)

plotLS(ls1, obs1, cat1)
mtext("Least Squares Learner", side = 3)

plotPed(weak3,obs3,cat1)

plotPed(strong3, obs3, cat1)

plotLS(ls3, obs3, cat1)

plotPed(weak5,obs5,cat1)

plotPed(strong5, obs5, cat1)

plotLS(ls5, obs5, cat1)
```
## 3 best rectangles

```{r}
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))

plotPed(weak1,obs1,cat1,nHypotheses = 3)
mtext("Weak Learner a = 0", side = 3)

plotPed(strong1, obs1, cat1, nHypotheses = 3)
mtext("Pedegogical Learner a = 1", side = 3)

plotLS(ls1, obs1, cat1, nHypotheses = 3)
mtext("Least Squares Learner", side = 3)

plotPed(weak3,obs3,cat1, nHypotheses = 3)

plotPed(strong3, obs3, cat1, nHypotheses = 3)

plotLS(ls3, obs3, cat1, nHypotheses = 3)

plotPed(weak5,obs5, cat1, nHypotheses = 3)

plotPed(strong5, obs5, cat1, nHypotheses = 3)

plotLS(ls5, obs5, cat1, nHypotheses = 3)
```


## 1 observation 

```{r}
obs1Pos = c(4,4,"positive")
names(obs1Pos) = c("x","y","category")
obs1Neg = c(8,8,"negative")
names(obs1Neg) = c("x","y","category")

strong1Pos = pedLearner(borders,obs1Pos)
strong1Neg = pedLearner(borders,obs1Neg)

par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(1,2))

plotPed(strong1Neg,obs1Neg,cat1, nHypotheses = 3)
plotPed(strong1Pos,obs1Pos,cat1, nHypotheses = 3)

```


pos = cbind(pX,pY,"positive")
## Need to find a  better way to sample negative evidence
neg = weakSampler(20)
neg = neg[neg[,"category"]== "negative",]
neg = neg[1:5,]
# set up different arrays with different numbers of observations
obs1 = rbind(pos[1,],neg[1,])
colnames(obs1) = c("x","y","category")
obs3 = rbind(pos[1:3,],neg[1:3,])
colnames(obs3) = c("x","y","category")
obs5 = rbind(pos[1:5,],neg[1:5,])
colnames(obs5) = c("x","y","category")
# visualize observations
plot(c(1,max(range)), c(1, max(range)), type= "n", xlab = "", ylab = "", main = "Hypothesis Space, Observations, and True Category Boundary")
rect(cat1[1],cat1[2],cat1[3],cat1[4],border = "red", lwd = 2)
points(obs5)
weak1 = pedLearner(borders, obs1, alpha = 0)
weak3 = pedLearner(borders, obs3, alpha = 0)
weak5 = pedLearner(borders, obs5, alpha = 0)
strong1 = pedLearner(borders, obs1)
strong3 = pedLearner(borders, obs3)
strong5 = pedLearner(borders, obs5)
ls1 = lsLearner(borders, obs1)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
library(here)
source(here("least-squares-heuristic.R"))
source(here("pedagogical-learner.R"))
source(here("functions.R"))
# Define range of possible features
range = 1:10
# set up all possible hypotheses
borders = makeBorders(range)
# Define the true category region
cat1 = c(2,2,6,8)
#visualize true category region
plot(c(1,max(range)), c(1, max(range)), type= "n", xlab = "", ylab = "")
rect(cat1[1],cat1[2],cat1[3],cat1[4],border = "red", lwd = 3)
nObs = 10
# Positive examples
pX = round(runif(nObs/2, cat1[1],cat1[3]),2) # X coordinates
pY = round(runif(nObs/2, cat1[2],cat1[4]),2) # Y coordinates
pos = cbind(pX,pY,"positive")
## Need to find a  better way to sample negative evidence
neg = weakSampler(20)
neg = neg[neg[,"category"]== "negative",]
neg = neg[1:5,]
# set up different arrays with different numbers of observations
obs1 = rbind(pos[1,],neg[1,])
colnames(obs1) = c("x","y","category")
obs3 = rbind(pos[1:3,],neg[1:3,])
colnames(obs3) = c("x","y","category")
obs5 = rbind(pos[1:5,],neg[1:5,])
colnames(obs5) = c("x","y","category")
# visualize observations
plot(c(1,max(range)), c(1, max(range)), type= "n", xlab = "", ylab = "", main = "Hypothesis Space, Observations, and True Category Boundary")
rect(cat1[1],cat1[2],cat1[3],cat1[4],border = "red", lwd = 2)
points(obs5)
weak1 = pedLearner(borders, obs1, alpha = 0)
weak3 = pedLearner(borders, obs3, alpha = 0)
weak5 = pedLearner(borders, obs5, alpha = 0)
strong1 = pedLearner(borders, obs1)
strong3 = pedLearner(borders, obs3)
strong5 = pedLearner(borders, obs5)
ls1 = lsLearner(borders, obs1)
ls3 = lsLearner(borders, obs3)
ls5 = lsLearner(borders, obs5)
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotPed(weak1,obs1,cat1)
mtext("Weak Learner a = 0", side = 3)
plotPed(strong1, obs1, cat1)
mtext("Pedegogical Learner a = 1", side = 3)
plotLS(ls1, obs1, cat1)
mtext("Least Squares Learner", side = 3)
plotPed(weak3,obs3,cat1)
plotPed(strong3, obs3, cat1)
plotLS(ls3, obs3, cat1)
plotPed(weak5,obs5,cat1)
plotPed(strong5, obs5, cat1)
plotLS(ls5, obs5, cat1)
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotPed(weak1,obs1,cat1,nHypotheses = 3)
mtext("Weak Learner a = 0", side = 3)
plotPed(strong1, obs1, cat1, nHypotheses = 3)
mtext("Pedegogical Learner a = 1", side = 3)
plotLS(ls1, obs1, cat1, nHypotheses = 3)
mtext("Least Squares Learner", side = 3)
plotPed(weak3,obs3,cat1, nHypotheses = 3)
plotPed(strong3, obs3, cat1, nHypotheses = 3)
plotLS(ls3, obs3, cat1, nHypotheses = 3)
plotPed(weak5,obs5, cat1, nHypotheses = 3
plotPed(strong5, obs5, cat1, nHypotheses = 3)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
library(here)
source(here("least-squares-heuristic.R"))
source(here("pedagogical-learner.R"))
source(here("functions.R"))
# Define range of possible features
range = 1:10
# set up all possible hypotheses
borders = makeBorders(range)
# Define the true category region
cat1 = c(2,2,6,8)
#visualize true category region
plot(c(1,max(range)), c(1, max(range)), type= "n", xlab = "", ylab = "")
rect(cat1[1],cat1[2],cat1[3],cat1[4],border = "red", lwd = 3)
nObs = 10
# Positive examples
pX = round(runif(nObs/2, cat1[1],cat1[3]),2) # X coordinates
pY = round(runif(nObs/2, cat1[2],cat1[4]),2) # Y coordinates
pos = cbind(pX,pY,"positive")
## Need to find a  better way to sample negative evidence
neg = weakSampler(20)
neg = neg[neg[,"category"]== "negative",]
neg = neg[1:5,]
# set up different arrays with different numbers of observations
obs1 = rbind(pos[1,],neg[1,])
colnames(obs1) = c("x","y","category")
obs3 = rbind(pos[1:3,],neg[1:3,])
colnames(obs3) = c("x","y","category")
obs5 = rbind(pos[1:5,],neg[1:5,])
colnames(obs5) = c("x","y","category")
# visualize observations
plot(c(1,max(range)), c(1, max(range)), type= "n", xlab = "", ylab = "", main = "Hypothesis Space, Observations, and True Category Boundary")
rect(cat1[1],cat1[2],cat1[3],cat1[4],border = "red", lwd = 2)
points(obs5)
weak1 = pedLearner(borders, obs1, alpha = 0)
weak3 = pedLearner(borders, obs3, alpha = 0)
weak5 = pedLearner(borders, obs5, alpha = 0)
strong1 = pedLearner(borders, obs1)
strong3 = pedLearner(borders, obs3)
strong5 = pedLearner(borders, obs5)
ls1 = lsLearner(borders, obs1)
ls3 = lsLearner(borders, obs3)
ls5 = lsLearner(borders, obs5)
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotPed(weak1,obs1,cat1)
mtext("Weak Learner a = 0", side = 3)
plotPed(strong1, obs1, cat1)
mtext("Pedegogical Learner a = 1", side = 3)
plotLS(ls1, obs1, cat1)
mtext("Least Squares Learner", side = 3)
plotPed(weak3,obs3,cat1)
plotPed(strong3, obs3, cat1)
plotLS(ls3, obs3, cat1)
plotPed(weak5,obs5,cat1)
plotPed(strong5, obs5, cat1)
plotLS(ls5, obs5, cat1)
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotPed(weak1,obs1,cat1,nHypotheses = 3)
mtext("Weak Learner a = 0", side = 3)
plotPed(strong1, obs1, cat1, nHypotheses = 3)
mtext("Pedegogical Learner a = 1", side = 3)
plotLS(ls1, obs1, cat1, nHypotheses = 3)
mtext("Least Squares Learner", side = 3)
plotPed(weak3,obs3,cat1, nHypotheses = 3)
plotPed(strong3, obs3, cat1, nHypotheses = 3)
plotLS(ls3, obs3, cat1, nHypotheses = 3)
plotPed(weak5,obs5, cat1, nHypotheses = 3)
plotPed(strong5, obs5, cat1, nHypotheses = 3)
plotLS(ls5, obs5, cat1, nHypotheses = 3)
obs1Pos = c(4,4,"positive")
names(obs1Pos) = c("x","y","category")
obs1Neg = c(8,8,"negative")
names(obs1Neg) = c("x","y","category")
strong1Pos = pedLearner(borders,obs1Pos)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
library(here)
source(here("least-squares-heuristic.R"))
source(here("pedagogical-learner.R"))
source(here("functions.R"))
# Define range of possible features
range = 1:10
# set up all possible hypotheses
borders = makeBorders(range)
# Define the true category region
cat1 = c(2,2,6,8)
#visualize true category region
plot(c(1,max(range)), c(1, max(range)), type= "n", xlab = "", ylab = "")
rect(cat1[1],cat1[2],cat1[3],cat1[4],border = "red", lwd = 3)
nObs = 10
# Positive examples
pX = round(runif(nObs/2, cat1[1],cat1[3]),2) # X coordinates
pY = round(runif(nObs/2, cat1[2],cat1[4]),2) # Y coordinates
pos = cbind(pX,pY,"positive")
## Need to find a  better way to sample negative evidence
neg = weakSampler(20)
neg = neg[neg[,"category"]== "negative",]
neg = neg[1:5,]
# set up different arrays with different numbers of observations
obs1 = rbind(pos[1,],neg[1,])
colnames(obs1) = c("x","y","category")
obs3 = rbind(pos[1:3,],neg[1:3,])
colnames(obs3) = c("x","y","category")
obs5 = rbind(pos[1:5,],neg[1:5,])
colnames(obs5) = c("x","y","category")
# visualize observations
plot(c(1,max(range)), c(1, max(range)), type= "n", xlab = "", ylab = "", main = "Hypothesis Space, Observations, and True Category Boundary")
rect(cat1[1],cat1[2],cat1[3],cat1[4],border = "red", lwd = 2)
points(obs5)
weak1 = pedLearner(borders, obs1, alpha = 0)
weak3 = pedLearner(borders, obs3, alpha = 0)
weak5 = pedLearner(borders, obs5, alpha = 0)
strong1 = pedLearner(borders, obs1)
strong3 = pedLearner(borders, obs3)
strong5 = pedLearner(borders, obs5)
ls1 = lsLearner(borders, obs1)
ls3 = lsLearner(borders, obs3)
ls5 = lsLearner(borders, obs5)
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotPed(weak1,obs1,cat1)
mtext("Weak Learner a = 0", side = 3)
plotPed(strong1, obs1, cat1)
mtext("Pedegogical Learner a = 1", side = 3)
plotLS(ls1, obs1, cat1)
mtext("Least Squares Learner", side = 3)
plotPed(weak3,obs3,cat1)
plotPed(strong3, obs3, cat1)
plotLS(ls3, obs3, cat1)
plotPed(weak5,obs5,cat1)
plotPed(strong5, obs5, cat1)
plotLS(ls5, obs5, cat1)
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotPed(weak1,obs1,cat1,nHypotheses = 3)
mtext("Weak Learner a = 0", side = 3)
plotPed(strong1, obs1, cat1, nHypotheses = 3)
mtext("Pedegogical Learner a = 1", side = 3)
plotLS(ls1, obs1, cat1, nHypotheses = 3)
mtext("Least Squares Learner", side = 3)
plotPed(weak3,obs3,cat1, nHypotheses = 3)
plotPed(strong3, obs3, cat1, nHypotheses = 3)
plotLS(ls3, obs3, cat1, nHypotheses = 3)
plotPed(weak5,obs5, cat1, nHypotheses = 3)
plotPed(strong5, obs5, cat1, nHypotheses = 3)
plotLS(ls5, obs5, cat1, nHypotheses = 3)
obs1Pos = c(4,4,"positive")
names(obs1Pos) = c("x","y","category")
obs1Neg = c(8,8,"negative")
names(obs1Neg) = c("x","y","category")
strong1Pos = pedLearner(borders,obs1Pos)
strong1Neg = pedLearner(borders,obs1Neg)
plotPed(strong1Neg,obs1Neg,cat1, nHypotheses = 3)
#plotPed(strong1Pos,obs1Pos,cat1, nHypotheses = 3)
View(strong1Pos)
debugonce(makeBorders)
# set up all possible hypotheses
borders = makeBorders(range)
# sets up all possible rectangles within a given hypothesis space
makeBorders = function(range){
# Create array with all possible rectangle coordinates within the range
borders = expand.grid(range,range,range,range)
for (i in 1:length(borders[,1])){ # replace duplicate rectangles with NA
if (borders[i,1]-borders[i,3] > 0 | borders[i,2]-borders[i,4] > 0){
borders[i,] = NA
}
}
borders = borders[complete.cases(borders),] # delete any rows with NA (rows that previously held duplicate rectangles)
# replace rows where size = 0 with NA
for (i in 1:length(borders[,1])){
if (borders[i,1] == borders[i,3] | borders[i,2] == borders[i,4]) {
borders[i,] = NA
}
}
borders = borders[complete.cases(borders),] # delete any rows with NA (rows that previously held rectangles with size 0)
return(borders)
}
# set up all possible hypotheses
borders = makeBorders(range)
strong1Pos = pedLearner(borders,obs1Pos)
View(strong1Pos)
plotPed(strong1Pos,obs1Pos,cat1, nHypotheses = 3)
plotPed(strong1Neg,obs1Neg,cat1, nHypotheses = 3)
plotPed(strong1Pos,obs1Pos,cat1, nHypotheses = 3)
learnerLik
learnerLik = function(observations,trueRecatngle,numPos = FALSE,numNeg = FALSE){
likelihood = c()
for(i in 1:length(observations[,1])){
if (observations[i,"category"] == "positive") {
likelihood[i] = (1 / findSize(trueRectangle) ^ numPos)
} else {
likelihood[i] = 1 / ((length(range)*length(range))-(findSize(trueRectangle)) ^ numPos)
}
}
return(likelihood)
}
test = pedLearner(cat1,observations)
test = pedLearner(cat1,obs1Pos)
test = pedLearner(borders,obs1Pos)
View(test)
View(strong1Pos)
# Set the learner's initial prior
prior = length(borders[,1]/sum(length(borders[,1])))
# Set the learner's initial prior
prior = 1:length(borders[,1]/sum(length(borders[,1])))
# Set the learner's initial prior
prior = 1:length(borders[,1]/sum(1:length(borders[,1])))
# Set the learner's initial prior
prior = 1:length(borders[,1])/sum(1:length(borders[,1]))
# Set the learner's initial prior
prior = rep(1,length(borders[,1]))/sum(rep(1,length(borders[,1])))
learner = pedLearner(trueRectangle,observations)
observations = obs1Pos
observation = obs1Pos
learner = pedLearner(trueRectangle,observation)
learner = pedLearner(borders,observation)
pointProb = learner[learner[,1] == trueRectangle[,1] & learner[,2] == trueRectangle[,2] & learner[,3] ==trueRectangle[,3] & learner[,1] ==trueRectangle[,1],"probability"]
pointProb = learner[learner[,1] == borders[,1] & learner[,2] == borders[,2] & learner[,3] ==borders[,3] & learner[,1] ==borders[,1],"probability"]
pointProb = learner[learner[,1] == borders[,1] & learner[,2] == borders[,2] & learner[,3] ==borders[,3] & learner[,4] == borders[,4],]
learner[,4]
borders[,4],
borders[,4]
pointProb = learner[learner[,1] == borders[,1],]
trueRectangle = cat1
pointProb = learner[learner[,1] == trueRectangle[,1] & learner[,2] == trueRectangle[,2] & learner[,3] == trueRectangle[,3] & learner[,4] == trueRectangle[,4],]
pointProb = learner[learner[,1] == trueRectangle[1] & learner[,2] == trueRectangle[2] & learner[,3] == trueRectangle[3] & learner[,4] == trueRectangle[4],]
View(pointProb)
cat1
pedTeacher = function(trueRectangle, range, borders){
allPoints = expand.grid(range,range) # set up all possible points
# label points as either positive or negative evidence
f = function(p) isInRectangle(r = trueRectangle, p = p) # alter isInRectangle function to be appropriate for apply() (include true rectangle)
positiveEvidence = apply(allPoints,1,f) # tells us if a point is in the rectangle or not
category = rep(NA,length(allPoints)) # empty column to fill with labels
observations = cbind(allPoints,category) # join into 1 matrix
for (i in 1:length(positiveEvidence)){
if (positiveEvidence[i] == TRUE){
observations[i,"category"] = "positive"
} else {
observations[i,"category"] = "negative"
}
}
# See which points maximize the pedagogical learner's posterior probability that the true rectangle is the best rectangle relative to the other rectangle
# Set the learner's initial prior
prior = rep(1,length(borders[,1]))/sum(rep(1,length(borders[,1]))) # uniform prior to start
# set up empty data frame to fill with the probability of the true rectangle given each point
probTrueRect = c()
# Calculate the learner's posterior:
for (i in 1:length(allPoints)){
learner = pedLearner(borders,observations[i,])
pointProb = learner[learner[,1] == trueRectangle[1] & learner[,2] == trueRectangle[2] & learner[,3] == trueRectangle[3] & learner[,4] == trueRectangle[4],"posterior"]
probTrueRect[i] = pointProb
}
# choose the point with the largest probability
maxProb = which.max(pointProb)
point = allPoints[maxProb,]
return(point)
}
test = pedTeacher(cat1,range, borders)
View(borders)
pedTeacher = function(trueRectangle, range){
# calculate all eligible possible rectangles within range
borders = makeBorders(range)
# set up all possible points
allPoints = expand.grid(range,range)
# label points as either positive or negative evidence
f = function(p) isInRectangle(r = trueRectangle, p = p) # alter isInRectangle function to be appropriate for apply() (include true rectangle)
positiveEvidence = apply(allPoints,1,f) # tells us if a point is in the rectangle or not
category = rep(NA,length(allPoints)) # empty column to fill with labels
observations = cbind(allPoints,category) # join into 1 matrix
for (i in 1:length(positiveEvidence)){
if (positiveEvidence[i] == TRUE){
observations[i,"category"] = "positive"
} else {
observations[i,"category"] = "negative"
}
}
# See which points maximize the pedagogical learner's posterior probability that the true rectangle is the best rectangle relative to the other rectangle
# Set the learner's initial prior
prior = rep(1,length(borders[,1]))/sum(rep(1,length(borders[,1]))) # uniform prior to start
# set up empty data frame to fill with the probability of the true rectangle given each point
probTrueRect = c()
# Calculate the learner's posterior:
for (i in 1:length(allPoints)){
learner = pedLearner(borders,observations[i,])
pointProb = learner[learner[,1] == trueRectangle[1] & learner[,2] == trueRectangle[2] & learner[,3] == trueRectangle[3] & learner[,4] == trueRectangle[4],"posterior"]
probTrueRect[i] = pointProb
}
# choose the point with the largest probability
maxProb = which.max(pointProb)
point = allPoints[maxProb,]
return(point)
}
test = pedTeacher(cat1,range)
pedTeacher = function(trueRectangle, range){
# calculate all eligible possible rectangles within range
borders = makeBorders(range)
# set up all possible points
allPoints = expand.grid(range,range)
# label points as either positive or negative evidence
f = function(p) isInRectangle(r = trueRectangle, p = p) # alter isInRectangle function to be appropriate for apply() (include true rectangle)
positiveEvidence = apply(allPoints,1,f) # tells us if a point is in the rectangle or not
category = rep(NA,length(allPoints)) # empty column to fill with labels
observations = cbind(allPoints,category) # join into 1 matrix
for (i in 1:length(positiveEvidence)){
if (positiveEvidence[i] == TRUE){
observations[i,"category"] = "positive"
} else {
observations[i,"category"] = "negative"
}
}
# See which points maximize the pedagogical learner's posterior probability that the true rectangle is the best rectangle relative to the other rectangle
# Set the learner's initial prior
prior = rep(1,length(borders[,1]))/sum(rep(1,length(borders[,1]))) # uniform prior to start
# set up empty data frame to fill with the probability of the true rectangle given each point
probTrueRect = c()
# Calculate the learner's posterior:
for (i in 1:length(allPoints)){
observation = allPonts[i,]
learner = pedLearner(borders,observation)
pointProb = learner[learner[,1] == trueRectangle[1] & learner[,2] == trueRectangle[2] & learner[,3] == trueRectangle[3] & learner[,4] == trueRectangle[4],"posterior"]
probTrueRect[i] = pointProb
}
# choose the point with the largest probability
maxProb = which.max(pointProb)
point = allPoints[maxProb,]
return(point)
}
test = pedLearner(borders,obs1Pos)
test = pedTeacher(cat1,range)
pedTeacher = function(trueRectangle, range){
# calculate all eligible possible rectangles within range
borders = makeBorders(range)
# set up all possible points
allPoints = expand.grid(range,range)
# label points as either positive or negative evidence
f = function(p) isInRectangle(r = trueRectangle, p = p) # alter isInRectangle function to be appropriate for apply() (include true rectangle)
positiveEvidence = apply(allPoints,1,f) # tells us if a point is in the rectangle or not
category = rep(NA,length(allPoints)) # empty column to fill with labels
observations = cbind(allPoints,category) # join into 1 matrix
for (i in 1:length(positiveEvidence)){
if (positiveEvidence[i] == TRUE){
observations[i,"category"] = "positive"
} else {
observations[i,"category"] = "negative"
}
}
# See which points maximize the pedagogical learner's posterior probability that the true rectangle is the best rectangle relative to the other rectangle
# Set the learner's initial prior
prior = rep(1,length(borders[,1]))/sum(rep(1,length(borders[,1]))) # uniform prior to start
# set up empty data frame to fill with the probability of the true rectangle given each point
probTrueRect = c()
# Calculate the learner's posterior:
for (i in 1:length(allPoints)){
observation = allPoints[i,]
learner = pedLearner(borders,observation)
pointProb = learner[learner[,1] == trueRectangle[1] & learner[,2] == trueRectangle[2] & learner[,3] == trueRectangle[3] & learner[,4] == trueRectangle[4],"posterior"]
probTrueRect[i] = pointProb
}
# choose the point with the largest probability
maxProb = which.max(pointProb)
point = allPoints[maxProb,]
return(point)
}
test = pedTeacher(cat1,range)
debugonce(pedTeacher)
test = pedTeacher(cat1,range)
debugonce(pedTeacher)
test = pedTeacher(cat1,range)
View(observations)
View(observations)
View(observations)
debugonce(pedTeacher)
test = pedTeacher(cat1,range)
debugonce(pedTeacher)
test = pedTeacher(cat1,range)
View(observation)
View(observation)
pedTeacher = function(trueRectangle, range){
# calculate all eligible possible rectangles within range
borders = makeBorders(range)
# set up all possible points
allPoints = expand.grid(range,range)
# label points as either positive or negative evidence
f = function(p) isInRectangle(r = trueRectangle, p = p) # alter isInRectangle function to be appropriate for apply() (include true rectangle)
positiveEvidence = apply(allPoints,1,f) # tells us if a point is in the rectangle or not
category = rep(NA,length(allPoints)) # empty column to fill with labels
observations = cbind(allPoints,category) # join into 1 matrix
for (i in 1:length(positiveEvidence)){
if (positiveEvidence[i] == TRUE){
observations[i,"category"] = "positive"
} else {
observations[i,"category"] = "negative"
}
}
# See which points maximize the pedagogical learner's posterior probability that the true rectangle is the best rectangle relative to the other rectangle
# Set the learner's initial prior
prior = rep(1,length(borders[,1]))/sum(rep(1,length(borders[,1]))) # uniform prior to start
# set up empty data frame to fill with the probability of the true rectangle given each point
probTrueRect = c()
# Calculate the learner's posterior:
for (i in 1:length(allPoints)){
observation = observations[i,]
learner = pedLearner(borders,observation)
pointProb = learner[learner[,1] == trueRectangle[1] & learner[,2] == trueRectangle[2] & learner[,3] == trueRectangle[3] & learner[,4] == trueRectangle[4],"posterior"]
probTrueRect[i] = pointProb
}
# choose the point with the largest probability
maxProb = which.max(pointProb)
point = allPoints[maxProb,]
return(point)
}
test = pedTeacher(cat1,range)
debugonce(pedTeacher)
test = pedTeacher(cat1,range)
debugonce(pedTeacher)
test = pedTeacher(cat1,range)
View(observation)
observation
#Var1 Var2 category
tmp = c(1,1, "negative")

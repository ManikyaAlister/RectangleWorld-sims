pX = round(runif(nObs/2, cat1[1],cat1[3]),2) # X coordinates
pY = round(runif(nObs/2, cat1[2],cat1[4]),2) # Y coordinates
pos = cbind(pX,pY,"1")
## Need to find a  better way to sample negative evidence
neg = weakSampler(20)
neg = neg[neg[,"category"]== "none",]
neg = neg[1:5,]
# set up different arrays with different numbers of observations
obs1 = rbind(pos[1,],neg[1,])
colnames(obs1) = c("x","y","category")
obs3 = rbind(pos[1:3,],neg[1:3,])
colnames(obs3) = c("x","y","category")
obs5 = rbind(pos[1:5,],neg[1:5,])
colnames(obs5) = c("x","y","category")
# visualize observations
plot(c(1,max(range)), c(1, max(range)), type= "n", xlab = "", ylab = "", main = "Hypothesis Space, Observations, and True Category Boundary")
rect(cat1[1],cat1[2],cat1[3],cat1[4],border = "blue", lwd = 3)
points(obs5)
weak1 = weakLearner(borders, obs1)
weak3 = weakLearner(borders, obs3)
weak5 = weakLearner(borders, obs5)
strong1 = strongLearner(borders, obs1)
# Strong sampling learner -- weights each hypothesis by its size, such that smaller hypotheses are allocated a higher probability
# ~ 1/(|h|^n), where |h| = size of the hypothesis, and n = number of observations
strongLearner = function(borders, observations) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
#logLikelihood = log(1 / findSize(weak) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
numPos = sum(observations[,"category"]=="1")
numNeg = sum(observations[,"category"]=="none")
likelihoodNeg = (1 / findSizeNeg(weak) ^ numNeg)
likelihoodPos = (1 / findSize(weak) ^ numPos) # non-log probability so that I can integrate it with alpha/transparency when plotting
likelihood = likelihoodNeg*likelihoodPos
posterior = likelihood/sum(likelihood)
strongHypotheses = cbind(weak, likelihood, logLikelihood,posterior)
colnames(strongHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
"likelihood",
"logLikelihood",
"posterior"
)
return(strongHypotheses)
}
strong1 = strongLearner(borders, obs1)
strong1 = strongLearner(borders, obs1)
strongLearner
# Strong sampling learner -- weights each hypothesis by its size, such that smaller hypotheses are allocated a higher probability
# ~ 1/(|h|^n), where |h| = size of the hypothesis, and n = number of observations
strongLearner = function(borders, observations) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
#logLikelihood = log(1 / findSize(weak) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
numPos = sum(observations[,"category"]=="1")
numNeg = sum(observations[,"category"]=="none")
likelihoodNeg = (1 / findSizeNeg(weak) ^ numNeg)
likelihoodPos = (1 / findSize(weak) ^ numPos) # non-log probability so that I can integrate it with alpha/transparency when plotting
likelihood = likelihoodNeg*likelihoodPos
posterior = likelihood/sum(likelihood)
logLikelihood = log(likelihood)
strongHypotheses = cbind(weak, likelihood, logLikelihood,posterior)
colnames(strongHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
"likelihood",
"logLikelihood",
"posterior"
)
return(strongHypotheses)
}
# Strong sampling learner -- weights each hypothesis by its size, such that smaller hypotheses are allocated a higher probability
# ~ 1/(|h|^n), where |h| = size of the hypothesis, and n = number of observations
strongLearner = function(borders, observations) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
#logLikelihood = log(1 / findSize(weak) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
numPos = sum(observations[,"category"]=="1")
numNeg = sum(observations[,"category"]=="none")
likelihoodNeg = (1 / findSizeNeg(weak) ^ numNeg)
likelihoodPos = (1 / findSize(weak) ^ numPos) # non-log probability so that I can integrate it with alpha/transparency when plotting
likelihood = likelihoodNeg*likelihoodPos
posterior = likelihood/sum(likelihood)
logLikelihood = log(likelihood)
strongHypotheses = cbind(weak, likelihood, logLikelihood,posterior)
colnames(strongHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
"likelihood",
"logLikelihood",
"posterior"
)
return(strongHypotheses)
}
strong1 = strongLearner(borders, obs1)
strong3 = strongLearner(borders, obs3)
strong5 = strongLearner(borders, obs5)
ls1 = lsLearner(borders, obs1)
ls3 = lsLearner(borders, obs3)
ls5 = lsLearner(borders, obs5)
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotWeak(weak1,obs1,cat1)
mtext("Weak Learner", side = 3)
plotStrong(strong1, obs1, cat1)
mtext("Strong Learner", side = 3)
plotLS(ls1, obs1, cat1)
mtext("Least Squares Learner", side = 3)
plotWeak(weak3,obs3,cat1)
plotStrong(strong3, obs3, cat1)
plotLS(ls3, obs3, cat1)
plotWeak(weak5,obs5,cat1)
plotStrong(strong5, obs5, cat1)
plotLS(ls5, obs5, cat1)
View(strong1)
View(strong5)
# Strong sampling learner -- weights each hypothesis by its size, such that smaller hypotheses are allocated a higher probability
# ~ 1/(|h|^n), where |h| = size of the hypothesis, and n = number of observations
strongLearner = function(borders, observations) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
#logLikelihood = log(1 / findSize(weak) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
numPos = sum(observations[,"category"]=="1")
numNeg = sum(observations[,"category"]=="none")
likelihoodNeg = (1 / findSizeNeg(weak) ^ numNeg)
likelihoodPos = (1 / findSize(weak) ^ numPos) # non-log probability so that I can integrate it with alpha/transparency when plotting
likelihood = likelihoodNeg*likelihoodPos
posterior = likelihood/sum(likelihood)
logLikelihood = log(likelihood)
strongHypotheses = cbind(weak[,1:6], likelihood, logLikelihood,posterior)
colnames(strongHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
"likelihood",
"logLikelihood",
"posterior"
)
return(strongHypotheses)
}
strong1 = strongLearner(borders, obs1)
strong3 = strongLearner(borders, obs3)
strong5 = strongLearner(borders, obs5)
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotWeak(weak1,obs1,cat1)
mtext("Weak Learner", side = 3)
plotStrong(strong1, obs1, cat1)
mtext("Strong Learner", side = 3)
plotLS(ls1, obs1, cat1)
mtext("Least Squares Learner", side = 3)
plotWeak(weak3,obs3,cat1)
plotStrong(strong3, obs3, cat1)
plotLS(ls3, obs3, cat1)
plotWeak(weak5,obs5,cat1)
plotStrong(strong5, obs5, cat1)
plotLS(ls5, obs5, cat1)
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,2))
plotStrong1(strong1, obs1, cat1)
# adjust plotting functions to jusrt show the highest probability rectangle
plotStrong1 = function(strongHypotheses, observations, categoryBoundary, range = 1:10){
strongHypotheses = strongHypotheses[which.max(strongHypotheses$posterior),]
plot(c(1,max(range)), c(1, max(range)), type= "n", xlab = "", ylab = "")
rect(strongHypotheses[,1],strongHypotheses[,2],strongHypotheses[,3],strongHypotheses[,4], col= rgb(0,0,1.0,alpha=.8), lwd = 0.01)
points(observations)
rect(categoryBoundary[1],categoryBoundary[2],categoryBoundary[3],categoryBoundary[4],border = "darkblue", lwd = 3)
}
plotLS1 = function(hypotheses,observations, categoryBoundary, range = 1:10) {
hypotheses = hypotheses[which.max(hypotheses$prob),]
plot(c(1,max(range)), c(1, max(range)), type= "n", xlab = "", ylab = "")
rect(hypotheses[,1],hypotheses[,2],hypotheses[,3],hypotheses[,4], col= rgb(0,0,1,alpha=.8),lwd = 0.01)
points(observations)
rect(categoryBoundary[1],categoryBoundary[2],categoryBoundary[3],categoryBoundary[4],border = "darkblue", lwd = 3)
}
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,2))
plotStrong1(strong1, obs1, cat1)
mtext("Strong Learner", side = 3)
plotLS1(ls1, obs1, cat1)
mtext("Least Squares Learner", side = 3)
plotStrong1(strong3, obs3, cat1)
plotLS1(ls3, obs3, cat1)
plotStrong1(strong5, obs5, cat1)
plotLS1(ls5, obs5, cat1)
obs1Pos = c(4,4)
obs1Neg = c(8,8)
strong1Pos = strongLearner(borders,obs1Pos)
obs1Pos = c(4,4,"1")
obs1Neg = c(8,8,"none")
strong1Pos = strongLearner(borders,obs1Pos)
obs1Pos = as.data.frame(c(4,4,"1"))
strong1Pos = strongLearner(borders,obs1Pos)
# Strong sampling learner -- weights each hypothesis by its size, such that smaller hypotheses are allocated a higher probability
# ~ 1/(|h|^n), where |h| = size of the hypothesis, and n = number of observations
strongLearner = function(borders, observations) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
#logLikelihood = log(1 / findSize(weak) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
if (is.vector(observations)){
numPos = sum(observations["category"]=="1")
numNeg = sum(observations["category"]=="none")
} else {
numPos = sum(observations[,"category"]=="1")
numNeg = sum(observations[,"category"]=="none")
}
likelihoodNeg = (1 / findSizeNeg(weak) ^ numNeg)
likelihoodPos = (1 / findSize(weak) ^ numPos) # non-log probability so that I can integrate it with alpha/transparency when plotting
likelihood = likelihoodNeg*likelihoodPos
posterior = likelihood/sum(likelihood)
logLikelihood = log(likelihood)
strongHypotheses = cbind(weak[,1:6], likelihood, logLikelihood,posterior)
colnames(strongHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
"likelihood",
"logLikelihood",
"posterior"
)
return(strongHypotheses)
}
obs1Pos = c(4,4,"1")
names(obs1Pos) = c("x2","y1","category")
obs1Neg = c(8,8,"none")
names(obs1Neg) = c("x2","y1","category")
obs1Pos
strong1Pos = strongLearner(borders,obs1Pos)
is.vector(obs1Pos)
# Strong sampling learner -- weights each hypothesis by its size, such that smaller hypotheses are allocated a higher probability
# ~ 1/(|h|^n), where |h| = size of the hypothesis, and n = number of observations
strongLearner = function(borders, observations) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
#logLikelihood = log(1 / findSize(weak) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
if (is.vector(observations)){
numPos = sum(observations["category"]=="1")
numNeg = sum(observations["category"]=="none")
} else {
numPos = sum(observations[,"category"]=="1")
numNeg = sum(observations[,"category"]=="none")
}
likelihoodNeg = (1 / findSizeNeg(weak) ^ numNeg)
likelihoodPos = (1 / findSize(weak) ^ numPos) # non-log probability so that I can integrate it with alpha/transparency when plotting
likelihood = likelihoodNeg*likelihoodPos
posterior = likelihood/sum(likelihood)
logLikelihood = log(likelihood)
strongHypotheses = cbind(weak[,1:6], likelihood, logLikelihood,posterior)
colnames(strongHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
"likelihood",
"logLikelihood",
"posterior"
)
return(strongHypotheses)
}
strong1Pos = strongLearner(borders,obs1Pos)
areInCat = function(borders,observations,catLabel) { # function that finds out which observations fall within a hypothesized category boundary (rectangle)
# borders = array of hypothesised category boundary points (coordinates of rectangles)
# observations = points that have been labelled as belonging to a certain category
# catLabel = the category that you're checking falls within a certain hypothesis space/category boundary/rectangle
if (is.vector(observations)){
observations = observations[observations["category"]==catLabel,]
} else {
observations = observations[observations[,"category"]==catLabel,]
}
if(length(observations) == 0) { # stop if there are no observations with specified catLabel
stop("There are no observations with that category label")
}
if (is.vector(observations)){ # figures out if there's only 1 observation
nObs = 1
} else {
nObs = length(observations[,1]) # if multiple observations, how many?
}
isInCat = array(dim = c(length(borders[, 1]), nObs)) # set up empty array to fill with whether a hypothesized category contains each observation
if (nObs == 1) { # if there's only one observation, check whether each hypothesis (rectangle) contains the observation
f = function(borders) isInRectangle(r = borders,  p = as.numeric(observations[c("x","y")]))
isInCat = apply(borders,1,f)
} else { # if there are multiple observations, loop through each observation and do above for each
f = function(borders) isInRectangle(r = borders, p = as.numeric(c(observations[i,"x"],observations[i,"y"])))
for (i in 1:nObs) {
isInCat[, i] = apply(borders,1,f)
}
}
return(isInCat)
}
strong1Pos = strongLearner(borders,obs1Pos)
obs1Pos = observations
observations = obs1Pos
observations = observations[observations["category"]==catLabel,]
View(weakLearner)
View(weakLearner)
View(weakSampler)
catLabel = 1
observations = observations[observations["category"]==catLabel,]
observations["category"]
observations["category"]==catLabel
observations = observations[observations["category"]==catLabel]
strong1Pos = strongLearner(borders,obs1Pos)
areInCat = function(borders,observations,catLabel) { # function that finds out which observations fall within a hypothesized category boundary (rectangle)
# borders = array of hypothesised category boundary points (coordinates of rectangles)
# observations = points that have been labelled as belonging to a certain category
# catLabel = the category that you're checking falls within a certain hypothesis space/category boundary/rectangle
if (is.vector(observations)){
observations = observations[observations["category"]==catLabel]
} else {
observations = observations[observations[,"category"]==catLabel,]
}
if(length(observations) == 0) { # stop if there are no observations with specified catLabel
stop("There are no observations with that category label")
}
if (is.vector(observations)){ # figures out if there's only 1 observation
nObs = 1
} else {
nObs = length(observations[,1]) # if multiple observations, how many?
}
isInCat = array(dim = c(length(borders[, 1]), nObs)) # set up empty array to fill with whether a hypothesized category contains each observation
if (nObs == 1) { # if there's only one observation, check whether each hypothesis (rectangle) contains the observation
f = function(borders) isInRectangle(r = borders,  p = as.numeric(observations[c("x","y")]))
isInCat = apply(borders,1,f)
} else { # if there are multiple observations, loop through each observation and do above for each
f = function(borders) isInRectangle(r = borders, p = as.numeric(c(observations[i,"x"],observations[i,"y"])))
for (i in 1:nObs) {
isInCat[, i] = apply(borders,1,f)
}
}
return(isInCat)
}
strong1Pos = strongLearner(borders,obs1Pos)
debugonce(stongLearner)
debugonce(stongLearner())
debugonce(stongLearner)
debugonce(strongLearner)
strong1Pos = strongLearner(borders,obs1Pos)
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,2))
plotStrong1(strong1, obs1, cat1)
mtext("Pedegogical Learner", side = 3)
plotLS1(ls1, obs1, cat1)
mtext("Least Squares Learner", side = 3)
plotStrong1(strong3, obs3, cat1)
plotLS1(ls3, obs3, cat1)
plotStrong1(strong5, obs5, cat1)
plotLS1(ls5, obs5, cat1)
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotWeak(weak1,obs1,cat1)
mtext("Weak Learner", side = 3)
plotStrong(strong1, obs1, cat1)
mtext("Pedegogical Learner", side = 3)
plotLS(ls1, obs1, cat1)
mtext("Least Squares Learner", side = 3)
plotWeak(weak3,obs3,cat1)
plotStrong(strong3, obs3, cat1)
plotLS(ls3, obs3, cat1)
plotWeak(weak5,obs5,cat1)
plotStrong(strong5, obs5, cat1)
plotLS(ls5, obs5, cat1)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
library(here)
source(here("functions.R"))
source(here("least-squares-heuristic.R"))
# Define range of possible features
range = 1:10
# Create array with all possible rectangle coordinates within the range
borders = expand.grid(range,range,range,range)
for (i in 1:length(borders[,1])){ # replace duplicate rectangles with NA
if (borders[i,1]-borders[i,3] > 0 | borders[i,2]-borders[i,4] > 0){
borders[i,] = NA
}
}
borders = borders[complete.cases(borders),] # delete rows that previously held duplicate rectangles (there was probably a better way to do this)
# Define the true category region
cat1 = c(2,2,6,8)
#visualize true category region
plot(c(1,max(range)), c(1, max(range)), type= "n", xlab = "", ylab = "")
rect(cat1[1],cat1[2],cat1[3],cat1[4],border = "blue", lwd = 3)
nObs = 10
# Positive examples
pX = round(runif(nObs/2, cat1[1],cat1[3]),2) # X coordinates
pY = round(runif(nObs/2, cat1[2],cat1[4]),2) # Y coordinates
pos = cbind(pX,pY,"1")
## Need to find a  better way to sample negative evidence
neg = weakSampler(20)
neg = neg[neg[,"category"]== "none",]
neg = neg[1:5,]
# set up different arrays with different numbers of observations
obs1 = rbind(pos[1,],neg[1,])
colnames(obs1) = c("x","y","category")
obs3 = rbind(pos[1:3,],neg[1:3,])
colnames(obs3) = c("x","y","category")
obs5 = rbind(pos[1:5,],neg[1:5,])
colnames(obs5) = c("x","y","category")
# visualize observations
plot(c(1,max(range)), c(1, max(range)), type= "n", xlab = "", ylab = "", main = "Hypothesis Space, Observations, and True Category Boundary")
rect(cat1[1],cat1[2],cat1[3],cat1[4],border = "blue", lwd = 3)
points(obs5)
weak1 = weakLearner(borders, obs1)
weak3 = weakLearner(borders, obs3)
weak5 = weakLearner(borders, obs5)
weak1 = weakLearner(borders, obs1)
weak3 = weakLearner(borders, obs3)
weak5 = weakLearner(borders, obs5)
strong1 = strongLearner(borders, obs1)
strong3 = strongLearner(borders, obs3)
strong5 = strongLearner(borders, obs5)
ls1 = lsLearner(borders, obs1)
ls3 = lsLearner(borders, obs3)
ls5 = lsLearner(borders, obs5)
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotWeak(weak1,obs1,cat1)
mtext("Weak Learner", side = 3)
plotStrong(strong1, obs1, cat1)
mtext("Pedegogical Learner", side = 3)
plotLS(ls1, obs1, cat1)
mtext("Least Squares Learner", side = 3)
plotWeak(weak3,obs3,cat1)
plotStrong(strong3, obs3, cat1)
plotLS(ls3, obs3, cat1)
plotWeak(weak5,obs5,cat1)
plotStrong(strong5, obs5, cat1)
plotLS(ls5, obs5, cat1)
View(strong5)
View(obs1)
View(strong1)
# Strong sampling learner -- weights each hypothesis by its size, such that smaller hypotheses are allocated a higher probability
# ~ 1/(|h|^n), where |h| = size of the hypothesis, and n = number of observations
strongLearner = function(borders, observations) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
#logLikelihood = log(1 / findSize(weak) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
if (is.vector(observations)){
numPos = sum(observations["category"]=="1")
numNeg = sum(observations["category"]=="none")
} else {
numPos = sum(observations[,"category"]=="1")
numNeg = sum(observations[,"category"]=="none")
}
likelihoodNeg = (1 / findSizeNeg(weak) ^ numNeg)
likelihoodPos = (1 / findSize(weak) ^ numPos) # non-log probability so that I can integrate it with alpha/transparency when plotting
likelihood = likelihoodNeg*likelihoodPos
posterior = likelihood/sum(likelihood)
logLikelihood = log(likelihood)
strongHypotheses = cbind(weak[,1:6], likelihood, likelihoodPos, likelihoodNeg,logLikelihood,posterior)
colnames(strongHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
"likelihood",
"likelihoodPos",
"likelihoodNeg",
"logLikelihood",
"posterior"
)
return(strongHypotheses)
}
strong1 = strongLearner(borders, obs1)
strong3 = strongLearner(borders, obs3)
strong5 = strongLearner(borders, obs5)
View(strong3)
View(strong1)
# Strong sampling learner -- weights each hypothesis by its size, such that smaller hypotheses are allocated a higher probability
# ~ 1/(|h|^n), where |h| = size of the hypothesis, and n = number of observations
strongLearner = function(borders, observations) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
#logLikelihood = log(1 / findSize(weak) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
if (is.vector(observations)){
numPos = sum(observations["category"]=="1")
numNeg = sum(observations["category"]=="none")
} else {
numPos = sum(observations[,"category"]=="1")
numNeg = sum(observations[,"category"]=="none")
}
likelihoodNeg = (1 / findSizeNeg(weak) ^ numNeg)
likelihoodPos = (1 / findSize(weak) ^ numPos) # non-log probability so that I can integrate it with alpha/transparency when plotting
likelihood = likelihoodNeg*likelihoodPos
posterior = likelihood/sum(likelihood)
logLikelihood = log(likelihood)
strongHypotheses = cbind(weak[,1:6], numPos,numNeg,likelihood, likelihoodPos, likelihoodNeg,logLikelihood,posterior)
colnames(strongHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
"numPos",
"numNeg",
"likelihood",
"likelihoodPos",
"likelihoodNeg",
"logLikelihood",
"posterior"
)
return(strongHypotheses)
}
strong1 = strongLearner(borders, obs1)
strong3 = strongLearner(borders, obs3)
strong5 = strongLearner(borders, obs5)

mtext("Strong Sampler", side = 3)
plotWeak(weak3,obs3,cat1)
plotStrong(strong3, obs3, cat1)
plotWeak(weak5,obs5,cat1)
plotStrong(strong5, obs5, cat1)
# Different levels of alpha parameter: 0,1,2
# 1 observation
ped1a0 = alphaPedegogicalLearner(borders,obs1,0)
ped1a1 = alphaPedegogicalLearner(borders,obs1,1)
ped1a2 = alphaPedegogicalLearner(borders,obs1,2)
# 3 observations
ped3a0 = alphaPedegogicalLearner(borders,obs3,0)
ped3a1 = alphaPedegogicalLearner(borders,obs3,1)
ped3a2 = alphaPedegogicalLearner(borders,obs3,2)
# 5 observations
ped5a0 = alphaPedegogicalLearner(borders,obs5,0)
ped5a1 = alphaPedegogicalLearner(borders,obs5,1)
ped5a2 = alphaPedegogicalLearner(borders,obs5,2)
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotStrong(ped1a0,obs1,cat1)
mtext("alpha = 0", side = 3)
plotStrong(ped1a1,obs1,cat1)
mtext("alpha = 1", side = 3)
plotStrong(ped1a2,obs1,cat1)
mtext("alpha = 2", side = 3)
plotStrong(ped3a0,obs3,cat1)
plotStrong(ped3a1,obs3,cat1)
plotStrong(ped3a2,obs3,cat1)
plotStrong(ped5a0,obs5,cat1)
plotStrong(ped5a1,obs5,cat1)
plotStrong(ped5a2,obs5,cat1)
# trect = rbind(c(1, 1, 7, 7), c(1, 8, 8, 9))
# isInRectangle(c(2, 9), trect)
# weakLearner(trect, obs5)
# areInCat(trect, obs5, "none")
#
# isInRectangle(c(2, 9), c(1, 1, 7, 7))
# isInRectangle(obs5[8, ], c(1, 8, 8, 9))
# isInRectangle(obs5[8, ], trect)
#
# tmp = c(2, 7, 4, 8)
# tmp1 = c(2, 7, 3, 9)
# findSize(tmp1)
View(strong5)
View(strong1)
View(ped5a1)
View(ped5a0)
View(ped5a1)
View(ped5a2)
alphaPedegogicalLearner = function(borders, observations, alphaParam) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
logLikelihood = alphaParam * log(1 / findSize(weak) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
#likelihood = (1 / findSize(weak) ^ length(observations[, 1]))^alphaParam# non-log probability so that I can integrate it with alpha when plotting.
posterior = logLikelihood/sum(logLikelihood)
# (cont. from above): I'm dividing them by the sum so that the generalization gradients show up a bit better in the plots. By summing these the likelihood acts more like a posterior probability, but since I'm not working with a prior at the moment it's basically the same.
strongHypotheses = cbind(weak, likelihood, logLikelihood,posterior)
colnames(strongHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
"likelihood",
"logLikelihood",
"posterior"
)
return(strongHypotheses)
}
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotStrong(ped1a0,obs1,cat1)
mtext("alpha = 0", side = 3)
plotStrong(ped1a1,obs1,cat1)
mtext("alpha = 1", side = 3)
plotStrong(ped1a2,obs1,cat1)
mtext("alpha = 2", side = 3)
plotStrong(ped3a0,obs3,cat1)
plotStrong(ped3a1,obs3,cat1)
plotStrong(ped3a2,obs3,cat1)
plotStrong(ped5a0,obs5,cat1)
plotStrong(ped5a1,obs5,cat1)
plotStrong(ped5a2,obs5,cat1)
View(ped3a1)
alphaPedegogicalLearner = function(borders, observations, alphaParam) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
logLikelihood = alphaParam * log((1 / findSize(weak)) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
#likelihood = (1 / findSize(weak) ^ length(observations[, 1]))^alphaParam# non-log probability so that I can integrate it with alpha when plotting.
posterior = logLikelihood/sum(logLikelihood)
# (cont. from above): I'm dividing them by the sum so that the generalization gradients show up a bit better in the plots. By summing these the likelihood acts more like a posterior probability, but since I'm not working with a prior at the moment it's basically the same.
strongHypotheses = cbind(weak, likelihood, logLikelihood,posterior)
colnames(strongHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
"likelihood",
"logLikelihood",
"posterior"
)
return(strongHypotheses)
}
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotStrong(ped1a0,obs1,cat1)
mtext("alpha = 0", side = 3)
plotStrong(ped1a1,obs1,cat1)
mtext("alpha = 1", side = 3)
plotStrong(ped1a2,obs1,cat1)
mtext("alpha = 2", side = 3)
plotStrong(ped3a0,obs3,cat1)
plotStrong(ped3a1,obs3,cat1)
plotStrong(ped3a2,obs3,cat1)
plotStrong(ped5a0,obs5,cat1)
plotStrong(ped5a1,obs5,cat1)
plotStrong(ped5a2,obs5,cat1)
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotPed(ped1a0,obs1,cat1)
mtext("alpha = 0", side = 3)
plotPed(ped1a1,obs1,cat1)
mtext("alpha = 1", side = 3)
plotPed(ped1a2,obs1,cat1)
mtext("alpha = 2", side = 3)
plotPed(ped3a0,obs3,cat1)
plotPed(ped3a1,obs3,cat1)
plotPed(ped3a2,obs3,cat1)
plotPed(ped5a0,obs5,cat1)
plotPed(ped5a1,obs5,cat1)
plotPed(ped5a2,obs5,cat1)
# Different levels of alpha parameter: 0,1,2
# 1 observation
ped1a0 = alphaPedegogicalLearner(borders,obs1,0)
alphaPedegogicalLearner = function(borders, observations, alphaParam) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
logLikelihood = alphaParam * log((1 / findSize(weak)) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
#likelihood = (1 / findSize(weak) ^ length(observations[, 1]))^alphaParam# non-log probability so that I can integrate it with alpha when plotting.
posterior = logLikelihood/sum(logLikelihood)
# (cont. from above): I'm dividing them by the sum so that the generalization gradients show up a bit better in the plots. By summing these the likelihood acts more like a posterior probability, but since I'm not working with a prior at the moment it's basically the same.
strongHypotheses = cbind(weak, likelihood, logLikelihood,posterior)
colnames(strongHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
"likelihood",
"logLikelihood",
"posterior"
)
return(strongHypotheses)
}
# Different levels of alpha parameter: 0,1,2
# 1 observation
ped1a0 = alphaPedegogicalLearner(borders,obs1,0)
alphaPedegogicalLearner = function(borders, observations, alphaParam) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
logLikelihood = alphaParam * log((1 / findSize(weak)) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
#likelihood = (1 / findSize(weak) ^ length(observations[, 1]))^alphaParam# non-log probability so that I can integrate it with alpha when plotting.
posterior = logLikelihood/sum(logLikelihood)
# (cont. from above): I'm dividing them by the sum so that the generalization gradients show up a bit better in the plots. By summing these the likelihood acts more like a posterior probability, but since I'm not working with a prior at the moment it's basically the same.
strongHypotheses = cbind(weak, likelihood, logLikelihood,posterior)
colnames(strongHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
#"likelihood",
"logLikelihood",
"posterior"
)
return(strongHypotheses)
}
# Different levels of alpha parameter: 0,1,2
# 1 observation
ped1a0 = alphaPedegogicalLearner(borders,obs1,0)
alphaPedegogicalLearner = function(borders, observations, alphaParam) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
logLikelihood = alphaParam * log((1 / findSize(weak)) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
#likelihood = (1 / findSize(weak) ^ length(observations[, 1]))^alphaParam# non-log probability so that I can integrate it with alpha when plotting.
posterior = logLikelihood/sum(logLikelihood)
# (cont. from above): I'm dividing them by the sum so that the generalization gradients show up a bit better in the plots. By summing these the likelihood acts more like a posterior probability, but since I'm not working with a prior at the moment it's basically the same.
strongHypotheses = cbind(weak, logLikelihood,posterior)
colnames(strongHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
#"likelihood",
"logLikelihood",
"posterior"
)
return(strongHypotheses)
}
# Different levels of alpha parameter: 0,1,2
# 1 observation
ped1a0 = alphaPedegogicalLearner(borders,obs1,0)
ped1a1 = alphaPedegogicalLearner(borders,obs1,1)
ped1a2 = alphaPedegogicalLearner(borders,obs1,2)
# 3 observations
ped3a0 = alphaPedegogicalLearner(borders,obs3,0)
ped3a1 = alphaPedegogicalLearner(borders,obs3,1)
ped3a2 = alphaPedegogicalLearner(borders,obs3,2)
# 5 observations
ped5a0 = alphaPedegogicalLearner(borders,obs5,0)
ped5a1 = alphaPedegogicalLearner(borders,obs5,1)
ped5a2 = alphaPedegogicalLearner(borders,obs5,2)
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotPed(ped1a0,obs1,cat1)
mtext("alpha = 0", side = 3)
plotPed(ped1a1,obs1,cat1)
mtext("alpha = 1", side = 3)
plotPed(ped1a2,obs1,cat1)
mtext("alpha = 2", side = 3)
plotPed(ped3a0,obs3,cat1)
plotPed(ped3a1,obs3,cat1)
plotPed(ped3a2,obs3,cat1)
plotPed(ped5a0,obs5,cat1)
plotPed(ped5a1,obs5,cat1)
plotPed(ped5a2,obs5,cat1)
alphaPedegogicalLearner = function(borders, observations, alphaParam) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
logLikelihood = alphaParam * log(1 / findSize(weak) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
#likelihood = (1 / findSize(weak) ^ length(observations[, 1]))^alphaParam# non-log probability so that I can integrate it with alpha when plotting.
posterior = logLikelihood/sum(logLikelihood)
# (cont. from above): I'm dividing them by the sum so that the generalization gradients show up a bit better in the plots. By summing these the likelihood acts more like a posterior probability, but since I'm not working with a prior at the moment it's basically the same.
strongHypotheses = cbind(weak, logLikelihood,posterior)
colnames(strongHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
#"likelihood",
"logLikelihood",
"posterior"
)
return(strongHypotheses)
}
# Different levels of alpha parameter: 0,1,2
# 1 observation
ped1a0 = alphaPedegogicalLearner(borders,obs1,0)
ped1a1 = alphaPedegogicalLearner(borders,obs1,1)
ped1a2 = alphaPedegogicalLearner(borders,obs1,2)
# 3 observations
ped3a0 = alphaPedegogicalLearner(borders,obs3,0)
ped3a1 = alphaPedegogicalLearner(borders,obs3,1)
ped3a2 = alphaPedegogicalLearner(borders,obs3,2)
# 5 observations
ped5a0 = alphaPedegogicalLearner(borders,obs5,0)
ped5a1 = alphaPedegogicalLearner(borders,obs5,1)
ped5a2 = alphaPedegogicalLearner(borders,obs5,2)
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotPed(ped1a0,obs1,cat1)
mtext("alpha = 0", side = 3)
plotPed(ped1a1,obs1,cat1)
mtext("alpha = 1", side = 3)
plotPed(ped1a2,obs1,cat1)
mtext("alpha = 2", side = 3)
plotPed(ped3a0,obs3,cat1)
plotPed(ped3a1,obs3,cat1)
plotPed(ped3a2,obs3,cat1)
plotPed(ped5a0,obs5,cat1)
plotPed(ped5a1,obs5,cat1)
plotPed(ped5a2,obs5,cat1)
View(ped1a0)
View(ped1a1)
alphaPedegogicalLearner = function(borders, observations, alphaParam) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
logLikelihood = alphaParam * log(1 / findSize(weak) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
#likelihood = (1 / findSize(weak) ^ length(observations[, 1]))^alphaParam# non-log probability so that I can integrate it with alpha when plotting.
posterior = logLikelihood/sum(logLikelihood)
# (cont. from above): I'm dividing them by the sum so that the generalization gradients show up a bit better in the plots. By summing these the likelihood acts more like a posterior probability, but since I'm not working with a prior at the moment it's basically the same.
pedHypotheses = cbind(weak, logLikelihood,posterior)
colnames(pedHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
#"likelihood",
"logLikelihood",
"posterior"
)
return(pedHypotheses)
}
# Different levels of alpha parameter: 0,1,2
# 1 observation
ped1a0 = alphaPedegogicalLearner(borders,obs1,0)
ped1a1 = alphaPedegogicalLearner(borders,obs1,1)
ped1a2 = alphaPedegogicalLearner(borders,obs1,2)
# 3 observations
ped3a0 = alphaPedegogicalLearner(borders,obs3,0)
ped3a1 = alphaPedegogicalLearner(borders,obs3,1)
ped3a2 = alphaPedegogicalLearner(borders,obs3,2)
# 5 observations
ped5a0 = alphaPedegogicalLearner(borders,obs5,0)
ped5a1 = alphaPedegogicalLearner(borders,obs5,1)
ped5a2 = alphaPedegogicalLearner(borders,obs5,2)
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotPed(ped1a0,obs1,cat1)
mtext("alpha = 0", side = 3)
plotPed(ped1a1,obs1,cat1)
mtext("alpha = 1", side = 3)
plotPed(ped1a2,obs1,cat1)
mtext("alpha = 2", side = 3)
plotPed(ped3a0,obs3,cat1)
plotPed(ped3a1,obs3,cat1)
plotPed(ped3a2,obs3,cat1)
plotPed(ped5a0,obs5,cat1)
plotPed(ped5a1,obs5,cat1)
plotPed(ped5a2,obs5,cat1)
alphaPedegogicalLearner = function(borders, observations, alphaParam) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
logLikelihood = alphaParam * log(1 / findSize(weak) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
likelihood = (1 / findSize(weak) ^ length(observations[, 1]))^alphaParam# non-log probability so that I can integrate it with alpha when plotting.
posterior = logLikelihood/sum(logLikelihood)
# (cont. from above): I'm dividing them by the sum so that the generalization gradients show up a bit better in the plots. By summing these the likelihood acts more like a posterior probability, but since I'm not working with a prior at the moment it's basically the same.
pedHypotheses = cbind(weak, likelihood,logLikelihood,posterior)
colnames(pedHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
#"likelihood",
"logLikelihood",
"posterior"
)
return(pedHypotheses)
}
# Different levels of alpha parameter: 0,1,2
# 1 observation
ped1a0 = alphaPedegogicalLearner(borders,obs1,0)
ped1a1 = alphaPedegogicalLearner(borders,obs1,1)
ped1a2 = alphaPedegogicalLearner(borders,obs1,2)
# 3 observations
ped3a0 = alphaPedegogicalLearner(borders,obs3,0)
ped3a1 = alphaPedegogicalLearner(borders,obs3,1)
ped3a2 = alphaPedegogicalLearner(borders,obs3,2)
# 5 observations
ped5a0 = alphaPedegogicalLearner(borders,obs5,0)
ped5a1 = alphaPedegogicalLearner(borders,obs5,1)
ped5a2 = alphaPedegogicalLearner(borders,obs5,2)
View(ped1a0)
View(ped1a1)
View(weak1)
alphaPedegogicalLearner = function(borders, observations, alphaParam) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
logLikelihood = alphaParam * log(1 / findSize(weak) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
likelihood = (1 / findSize(weak) ^ length(observations[, 1]))^alphaParam# non-log probability so that I can integrate it with alpha when plotting.
posterior = logLikelihood/sum(logLikelihood)
# (cont. from above): I'm dividing them by the sum so that the generalization gradients show up a bit better in the plots. By summing these the likelihood acts more like a posterior probability, but since I'm not working with a prior at the moment it's basically the same.
pedHypotheses = cbind(weak[,1:6], likelihood,logLikelihood,posterior)
colnames(pedHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
#"likelihood",
"logLikelihood",
"posterior"
)
return(pedHypotheses)
}
# Different levels of alpha parameter: 0,1,2
# 1 observation
ped1a0 = alphaPedegogicalLearner(borders,obs1,0)
ped1a1 = alphaPedegogicalLearner(borders,obs1,1)
ped1a2 = alphaPedegogicalLearner(borders,obs1,2)
# 3 observations
ped3a0 = alphaPedegogicalLearner(borders,obs3,0)
ped3a1 = alphaPedegogicalLearner(borders,obs3,1)
ped3a2 = alphaPedegogicalLearner(borders,obs3,2)
# 5 observations
ped5a0 = alphaPedegogicalLearner(borders,obs5,0)
ped5a1 = alphaPedegogicalLearner(borders,obs5,1)
ped5a2 = alphaPedegogicalLearner(borders,obs5,2)
alphaPedegogicalLearner = function(borders, observations, alphaParam) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
logLikelihood = alphaParam * log(1 / findSize(weak) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
likelihood = (1 / findSize(weak) ^ length(observations[, 1]))^alphaParam# non-log probability so that I can integrate it with alpha when plotting.
posterior = logLikelihood/sum(logLikelihood)
# (cont. from above): I'm dividing them by the sum so that the generalization gradients show up a bit better in the plots. By summing these the likelihood acts more like a posterior probability, but since I'm not working with a prior at the moment it's basically the same.
pedHypotheses = cbind(weak[,1:6], likelihood,logLikelihood,posterior)
colnames(pedHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
#"likelihood",
"logLikelihood",
"posterior"
)
return(pedHypotheses)
}
# Different levels of alpha parameter: 0,1,2
# 1 observation
ped1a0 = alphaPedegogicalLearner(borders,obs1,0)
ped1a1 = alphaPedegogicalLearner(borders,obs1,1)
ped1a2 = alphaPedegogicalLearner(borders,obs1,2)
# 3 observations
ped3a0 = alphaPedegogicalLearner(borders,obs3,0)
ped3a1 = alphaPedegogicalLearner(borders,obs3,1)
ped3a2 = alphaPedegogicalLearner(borders,obs3,2)
# 5 observations
ped5a0 = alphaPedegogicalLearner(borders,obs5,0)
ped5a1 = alphaPedegogicalLearner(borders,obs5,1)
ped5a2 = alphaPedegogicalLearner(borders,obs5,2)
View(ped1a2)
alphaPedegogicalLearner = function(borders, observations, alphaParam) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
logLikelihood = alphaParam * log(1 / findSize(weak) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
likelihood = (1 / findSize(weak) ^ length(observations[, 1]))^alphaParam# non-log probability so that I can integrate it with alpha when plotting.
posterior = logLikelihood/sum(logLikelihood)
# (cont. from above): I'm dividing them by the sum so that the generalization gradients show up a bit better in the plots. By summing these the likelihood acts more like a posterior probability, but since I'm not working with a prior at the moment it's basically the same.
pedHypotheses = cbind(weak[,1:6], likelihood,logLikelihood,posterior)
colnames(pedHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
"likelihood",
"logLikelihood",
"posterior"
)
return(pedHypotheses)
}
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotPed(ped1a0,obs1,cat1)
mtext("alpha = 0", side = 3)
plotPed(ped1a1,obs1,cat1)
# Different levels of alpha parameter: 0,1,2
# 1 observation
ped1a0 = alphaPedegogicalLearner(borders,obs1,0)
ped1a1 = alphaPedegogicalLearner(borders,obs1,1)
ped1a2 = alphaPedegogicalLearner(borders,obs1,2)
# 3 observations
ped3a0 = alphaPedegogicalLearner(borders,obs3,0)
ped3a1 = alphaPedegogicalLearner(borders,obs3,1)
ped3a2 = alphaPedegogicalLearner(borders,obs3,2)
# 5 observations
ped5a0 = alphaPedegogicalLearner(borders,obs5,0)
ped5a1 = alphaPedegogicalLearner(borders,obs5,1)
ped5a2 = alphaPedegogicalLearner(borders,obs5,2)
View(ped1a1)
View(ped1a0)
plotPed(ped1a1,obs1,cat1)
plotPed(ped3a0,obs3,cat1)
plotPed(ped3a1,obs3,cat1)
plotPed(ped3a2,obs3,cat1)
alphaPedegogicalLearner = function(borders, observations, alphaParam) {
weak = weakLearner(borders, observations) # builds off of the weak learner because it's just one extra step
#logLikelihood = alphaParam * log(1 / findSize(weak) ^ length(observations[, 1])) # operationalising n, number of observations, as both positive and negative observations
likelihood = (1 / findSize(weak) ^ length(observations[, 1]))^alphaParam# non-log probability so that I can integrate it with alpha when plotting.
posterior = likelihood/sum(likelihood)
# (cont. from above): I'm dividing them by the sum so that the generalization gradients show up a bit better in the plots. By summing these the likelihood acts more like a posterior probability, but since I'm not working with a prior at the moment it's basically the same.
pedHypotheses = cbind(weak[,1:6], likelihood,posterior)
colnames(pedHypotheses) = c(
"x1",
"y1",
"x2",
"y2",
"positiveEvidence",
"negativeEvidence",
"likelihood",
#"logLikelihood",
"posterior"
)
return(pedHypotheses)
}
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotPed(ped1a0,obs1,cat1)
# Different levels of alpha parameter: 0,1,2
# 1 observation
ped1a0 = alphaPedegogicalLearner(borders,obs1,0)
ped1a1 = alphaPedegogicalLearner(borders,obs1,1)
ped1a2 = alphaPedegogicalLearner(borders,obs1,2)
# 3 observations
ped3a0 = alphaPedegogicalLearner(borders,obs3,0)
ped3a1 = alphaPedegogicalLearner(borders,obs3,1)
ped3a2 = alphaPedegogicalLearner(borders,obs3,2)
# 5 observations
ped5a0 = alphaPedegogicalLearner(borders,obs5,0)
ped5a1 = alphaPedegogicalLearner(borders,obs5,1)
ped5a2 = alphaPedegogicalLearner(borders,obs5,2)
par(omi=rep(0.3, 4), mar=c(1,1,1,1), mfrow=c(3,3))
plotPed(ped1a0,obs1,cat1)
mtext("alpha = 0", side = 3)
plotPed(ped1a1,obs1,cat1)
mtext("alpha = 1", side = 3)
plotPed(ped1a2,obs1,cat1)
mtext("alpha = 2", side = 3)
plotPed(ped3a0,obs3,cat1)
plotPed(ped3a1,obs3,cat1)
plotPed(ped3a2,obs3,cat1)
plotPed(ped5a0,obs5,cat1)
plotPed(ped5a1,obs5,cat1)
plotPed(ped5a2,obs5,cat1)

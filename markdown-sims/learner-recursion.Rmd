---
title: "Extra Learner Recursion"
author: "Manikya Aluster"
date: "2023-02-15"
output: html_document
---
```{r}
# We'll begin by loading up the libraries and data we need, as always.
knitr::opts_chunk$set(echo = TRUE)
 rm(list=ls())
 
# loading the libraries
library(tidyverse)
library(here)
library(ggplot2)
library(ggpubr)
library(nnet)

# size of hypothesis space
H <- 3
# version of this we are running (for saving)
version <- 0
# the alpha the learner assumes the teacher is following
lnAlpha <- -1
# the alpha the teacher is actually following
tchAlpha <- -1
# the alpha the teacher is assuming the learner thinks the teacher is following 
tchLnAlpha <- -1
# what size of rectangle this pair is using (small, medium, or large)
trueRectSize <- "small"
# number of best hypotheses to show
nBestH <- 3
# TRUE if we want the teacher to always sample the highest probability pt
# (if multiple points are equally high probability, randomly selects among them)
maximise <- TRUE

# sourcing our files
source(here("genericFunctions.R"))
source(here("calculatingFunctions.R"))
source(here("plottingFunctions.R"))

# load the data
fileSeg <- paste0("x0to",H,"y0to",H)
fn <- paste0("datafiles/",fileSeg,".RData")
load(here(fn))

# set alphas based on parameters above
lA <- which(alphas==lnAlpha)
lnAlphaText <- returnAlpha(lnAlpha)
tA <- which(alphas==tchAlpha)
tchAlphaText <- returnAlpha(tchAlpha)
tlA <- which(alphas==tchLnAlpha)
tchLnAlphaText <- returnAlpha(tchLnAlpha)

mx <- "F"
if (maximise) {
   mx <- "T" 
}

```

Here are some functions I have made to try and figure this out: 

```{r}
# set up way to get indexes of points
pts$index = 1:length(pts[,1])
getPtIndex = function(pt){
  index = pts[pts[,"x"] == pt[,"x"] & pts[,"y"] == pt[,"y"], "index"]
  index
}

# If there is already a point X hypothesis distribution, this function attempts to apply a new 
# point X hypothesis distribution on top of that with new alpha. 
updateProbabilityOfPoints = function(hyp,pts,whichObs,alpha=0, ptDist){
  nHyp <- nrow(hyp)
  nPts <- nrow(pts)
  consPt <- matrix(0,nrow=nPts,ncol=nHyp)
  rownames(consPt) <- rownames(pts)
  colnames(consPt) <- rownames(hyp)
  
  for (p in 1:nPts) {
    for (h in 1:nHyp) {
      if (isInRectangle(c(pts[p,1],pts[p,2]),c(hyp$x1[h],hyp$y1[h],hyp$x2[h],hyp$y2[h]))) {
        if (whichObs=="pos") {
          consPt[p,h] <- ptDist[p,h]*(1/hyp$size[h])^alpha # likelihood
        } 
      } else {
        if (whichObs=="neg") {
          consPt[p,h] <- ptDist[p,h]*(1/hyp$negSize[h])^alpha 
        }
      }
    }
  }
  
  consPt <- data.frame(consPt) 
  return(consPt)
}

# Function to get the point X hypothesis matrix pTeacher(d|h)
updatePointHypMatrix = function(alpha, hyp, pts, ptDist, whichObs = "all"){
  ppp <- updateProbabilityOfPoints(hyp,pts,whichObs="pos",alpha=alpha,ptDist)
  nnn <- updateProbabilityOfPoints(hyp,pts,whichObs="neg",alpha=alpha, ptDist)
  aaa <- ppp + nnn
  if(whichObs == "pos") {
      matrix <- data.matrix(ppp/sum(ppp))
  } else if (whichObs == "neg") {
      matrix <- data.matrix(nnn/sum(nnn))
  } else if (whichObs == "all") {
      matrix <- data.matrix(aaa/sum(aaa))
  }
  matrix
}

# function to plot hypothesis distribution by size
plotHypDist = function(hyp, title){
  hyp %>%
    filter(posterior>0) %>%
    arrange(size) %>%
    ggplot(main = title)+
    geom_line(aes(x = size, y = posterior))+
    xlim(c(1,9))+
    ggtitle(title)
}
```


```{r echo=FALSE, fig.height=5, fig.width=12, fig.align="center", message=FALSE, warning=FALSE}
lnPts <- pts
tchPts <- pts
lnHyp <- hyp
tchHyp <- hyp


# Step 1: Learner observes a point
newPt <- data.frame(x = 1.5, y = 1.5, category = "positive")
newPt$index = getPtIndex(newPt)
```

After seeing the point, the learner needs to infer what the teacher thinks their hypothesis distribution would look like after seeing it, since they would have chosen a point that would minimize the learner's belief in the true hypothesis. 

```{r}
# Step 2: Learner infers the hypotheses distribution the teacher thinks the learner has initially. 
tchLnHyp1 <- updateHypotheses(allProbPts[,,tlA],consPts,newPt,tchHyp)
plotHypDist(tchLnHyp1, "Teacher's hypothesis distribution for the learner")
```

The above plot shows that the teacher's initial assumptions is that because the learner is suspicious, they are more likely to believe that the hypothesis is large given the point that appeared. 

Given that the teacher knows this information, the learner needs to infer how probable each point actually is, given the teachers goals (which in this case is to deceive the teacher)

So the learner is going to try to calculate pTeacher(d|h), which is the probability of each data point given each hypothesis. They'll do this by feeding in the point distribution the teacher thinks the learner has, then apply the transformation that the teacher would do to try and mislead them. 

The learner can then use that to figure out the probability of each hypothesis given the data, pLearner(h|d).


```{r}
# Learner takes the probability of each point given each hypothesis the teacher thinks they have
tchPtsMatrix <- posProbPts[,,tlA]

# Then updates that matrix one more time, applying alpha again to infer what the actual probability over points. 
lnPtsMatrix <- updatePointHypMatrix(lnAlpha, tchHyp, pts, tchPtsMatrix, "pos")
pNewPt <- lnPtsMatrix[newPt$index,]

# convert into posterior probability
lnTchHyp <- tchHyp %>%
  mutate(posterior = pNewPt/sum(pNewPt))

plotHypDist(lnTchHyp, "Learner's actual distribution over hypotheses")


```

This isn't right because if the recursion is happening properly it should reduce the learner's belief in the larger hypotheses, but this makes it even stronger. This is what I've been consistently stuck on. 